{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: neo4j in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (5.20.0)\n",
      "Requirement already satisfied: openai in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.30.1)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.1.60)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from neo4j) (2022.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain neo4j openai wikipedia tiktoken langchain_openai pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (5.20.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from neo4j) (2022.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import neo4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neo4j connection\n",
    "url = \"neo4j+s://e1557b9c.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"m8CFT88LsU8EZ9BzYC4dC0c7Pwgc1ZBQxHKqQKpA3HY\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain for graph\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# The base URL for your Azure OpenAI resource.\n",
    "openai_api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# API version e.g. \"2023-07-01-preview\"\n",
    "openai_api_version = config_details['OPENAI_API_VERSION']\n",
    "\n",
    "# The name of your Azure OpenAI deployment chat model. e.g. \"gpt-35-turbo-0613\"\n",
    "deployment_name = config_details['DEPLOYMENT_NAME']\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is set to `azure`\n",
    "openai_api_type = \"azure\"\n",
    "\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Create an instance of chat llm\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=openai_api_base,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=deployment_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type=openai_api_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "from langchain.llms import OpenAI\n",
    "from typing import Optional, List\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "      \"system\",\n",
    "      f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a specialized algorithm designed to extract structured data from Wikipedia articles to build a comprehensive knowledge graph about banks in Malaysia.\n",
    "- **Nodes** represent entities such as bank names, key people, locations, services, and other relevant concepts.\n",
    "- The goal is to structure information in a manner that highlights relationships, organizational structure, historical facts, and key events.\n",
    "\n",
    "## 2. Sections and Nodes\n",
    "- **Sections**: Target key sections such as 'History', 'Services', 'Key People', 'Locations', etc. Identify relevant data and organizational details.\n",
    "- **Entities and Concepts**: Create nodes for identifiable entities like bank names, board members, services, and historical events.\n",
    "- **Node IDs**: Use names or specific identifiers for node labels. Avoid integers or vague references.\n",
    "- **Relationships**: Map relationships that reflect organizational structure, historical events, service offerings, and key personnel.\n",
    "\n",
    "## 3. Detailed Extraction\n",
    "- **Extract Details**: Focus on key facts and information, converting them into properties of nodes. For instance, the founding date of a bank should be attached to the 'History' node.\n",
    "- **Accuracy and Precision**: Ensure the extraction of data is accurate, citing exact figures and contextual information.\n",
    "- **Contextual Relevance**: Attach relevant notes and details from different sections to the appropriate entities or events.\n",
    "\n",
    "## 4. Compliance and Consistency\n",
    "- **Consistency**: Use consistent terminology across different sections of the article to avoid confusion.\n",
    "- **Strict Compliance**: Adhere strictly to the rules for knowledge graph construction.\n",
    "\n",
    "## 5. Symbolic Tokens and Special Characters\n",
    "- **Symbolic Tokens**: Identify and properly handle symbolic tokens (e.g., %, $, etc.) and ensure they are accurately represented in the data.\n",
    "- **Special Characters**: Recognize special characters and their meanings (e.g., ± for approximately, > for greater than, etc.) and handle them appropriately in the extraction process.\n",
    "\n",
    "## 6. Domain Vocabulary and Abbreviations\n",
    "- **Domain Vocabulary**: Utilize and recognize domain-specific vocabulary and terminology relevant to the banking industry.\n",
    "- **Abbreviations**: Identify and expand domain-related abbreviations (e.g., CEO for Chief Executive Officer) to ensure clarity and accuracy.\n",
    "\n",
    "## 7. N-gram Extraction\n",
    "- **N-grams**: Extract meaningful N-grams (bigrams, trigrams, etc.) that represent significant terms or phrases and include them as part of the node properties.\n",
    "\n",
    "## 8. Tips\n",
    "- Remember to format data as attributes of the nodes and structure the graph to reflect the organization and flow of the Wikipedia article.\n",
    "- Use the given format to extract information from the following input: <input here>\n",
    "- Tip: Make sure to answer in the correct format\n",
    "\"\"\"),\n",
    "        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "        (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "    ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes: Optional[List[str]] = None,\n",
    "    rels: Optional[List[str]] = None\n",
    ") -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document)['function']\n",
    "    \n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "        nodes=[map_to_base_node(node) for node in data.nodes],\n",
    "        relationships=[map_to_base_relationship(rel) for rel in data.rels],\n",
    "        source=document.dict()  # Convert Document object to dictionary\n",
    "    )\n",
    "    \n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WendyChuaXingZhao\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\WendyChuaXingZhao\\anaconda3\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# List of Wikipedia queries\n",
    "queries = [\"Maybank\", \"CIMB\", \"Public Bank\"]\n",
    "\n",
    "# Load Wikipedia articles\n",
    "documents = []\n",
    "for query in queries:\n",
    "    raw_documents = WikipediaLoader(query=query).load()\n",
    "    documents.extend(raw_documents)\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "\n",
    "# Split documents into manageable chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Combine chunks into a single text\n",
    "text = \" \".join([chunk.page_content for chunk in chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [26:07<00:00, 21.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    extract_and_store_graph(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
