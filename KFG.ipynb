{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: neo4j in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (5.20.0)\n",
      "Requirement already satisfied: openai in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.30.1)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.1.60)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from neo4j) (2022.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain neo4j openai wikipedia tiktoken langchain_openai pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package Imports\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neo4j connection\n",
    "url = \"neo4j+s://e1557b9c.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"m8CFT88LsU8EZ9BzYC4dC0c7Pwgc1ZBQxHKqQKpA3HY\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain for graph\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# The base URL for your Azure OpenAI resource.\n",
    "openai_api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# API version e.g. \"2023-07-01-preview\"\n",
    "openai_api_version = config_details['OPENAI_API_VERSION']\n",
    "\n",
    "# The name of your Azure OpenAI deployment chat model. e.g. \"gpt-35-turbo-0613\"\n",
    "deployment_name = config_details['DEPLOYMENT_NAME']\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is set to `azure`\n",
    "openai_api_type = \"azure\"\n",
    "\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Create an instance of chat llm\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=openai_api_base,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=deployment_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type=openai_api_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "      \"system\",\n",
    "      f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a specialized algorithm designed to extract structured financial data from bank annual reports to build a comprehensive knowledge graph.\n",
    "- **Nodes** represent financial terms, entities, departments, and concepts specific to the banking industry.\n",
    "- The goal is to structure information in a manner that highlights financial relationships, decision-making hierarchies, and policy summaries.\n",
    "\n",
    "## 2. Sections and Nodes\n",
    "- **Sections**: Target key sections such as 'Director's Report', 'Statements of Financial Position', 'Income Statement', etc. Identify relevant financial data and decisions.\n",
    "- **Entities and Concepts**: Create nodes for identifiable entities like board members, financial metrics, and policy terms.\n",
    "- **Node IDs**: Use names or specific identifiers for node labels. Avoid integers or vague references.\n",
    "- **Relationships**: Map relationships that reflect financial dependencies, reporting structures, and policy impacts.\n",
    "\n",
    "## 3. Detailed Financial Extraction\n",
    "- **Extract Details**: Focus on numbers and financial statements, converting them into properties of nodes. For instance, assets and liabilities figures should be attached to the 'Statement of Financial Position' node.\n",
    "- **Accuracy and Precision**: Ensure the extraction of financial data is accurate, citing exact figures and contextual information.\n",
    "- **Contextual Relevance**: Attach relevant notes and assumptions from the 'Notes to Financial Statements' to the appropriate financial statements or metrics.\n",
    "\n",
    "## 4. Compliance and Consistency\n",
    "- **Regulatory Statements**: Extract and highlight compliance statements from 'Independent Auditors' Report' and 'Statutory Declaration'.\n",
    "- **Maintain Consistency**: Use consistent terminology across different sections of the report to avoid confusion.\n",
    "- **Strict Compliance**: Adhere strictly to the rules for knowledge graph construction.\n",
    "\n",
    "## 5. Symbolic Tokens and Special Characters\n",
    "- **Symbolic Tokens**: Identify and properly handle symbolic tokens (e.g., %, $, £, etc.) and ensure they are accurately represented in the data.\n",
    "- **Special Characters**: Recognize special characters and their meanings (e.g., ± for approximately, > for greater than, etc.) and handle them appropriately in the extraction process.\n",
    "\n",
    "## 6. Domain Vocabulary and Abbreviations\n",
    "- **Domain Vocabulary**: Utilize and recognize domain-specific vocabulary and terminology relevant to the banking and financial industry.\n",
    "- **Abbreviations**: Identify and expand domain-related abbreviations (e.g., FVOCI for Fair Value through Other Comprehensive Income) to ensure clarity and accuracy.\n",
    "\n",
    "## 7. N-gram Extraction\n",
    "- **N-grams**: Extract meaningful N-grams (bigrams, trigrams, etc.) that represent significant financial terms or phrases and include them as part of the node properties.\n",
    "\n",
    "## 8. Tips\n",
    "- Remember to format financial data as attributes of the nodes and structure the graph to reflect the organization and flow of the annual report.\n",
    "- Use the given format to extract information from the following input: <input here>\n",
    "- Tip: Make sure to answer in the correct format\n",
    "\"\"\"),\n",
    "        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "        (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "    ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes: Optional[List[str]] = None,\n",
    "    rels: Optional[List[str]] = None\n",
    ") -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document)['function']\n",
    "    \n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "        nodes=[map_to_base_node(node) for node in data.nodes],\n",
    "        relationships=[map_to_base_relationship(rel) for rel in data.rels],\n",
    "        source=document.dict()  # Convert Document object to dictionary\n",
    "    )\n",
    "    \n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: keras in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: torch in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow keras torch transformers spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import pickle\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load FinBERT model for NER\n",
    "model_checkpoint = \"Kansallisarkisto/finbert-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\", framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Define the folder containing the PDF files and the ground truth folder\n",
    "pdf_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\Financial Documents\"\n",
    "ground_truth_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\ground_truth\"\n",
    "\n",
    "# Create ground truth folder if it doesn't exist\n",
    "os.makedirs(ground_truth_folder_path, exist_ok=True)\n",
    "\n",
    "def extract_text_and_tables_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = ''\n",
    "            tables = []\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text\n",
    "                page_tables = page.extract_tables()\n",
    "                if page_tables:\n",
    "                    tables.extend(page_tables)\n",
    "        return full_text, tables\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF {pdf_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove noise\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]+', ' ', text)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stop-word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "def extract_entities_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {'PERSON', 'ORG', 'GPE', 'LOC', 'DATE', 'MONEY', 'PERCENT'}]\n",
    "    return entities\n",
    "\n",
    "def extract_entities_finbert(text):\n",
    "    ner_results = token_classifier(text)\n",
    "    entities = [entity['word'] for entity in ner_results]\n",
    "    return entities\n",
    "\n",
    "def combine_and_postprocess_entities(spacy_entities, finbert_entities):\n",
    "    # Combine entities from both models\n",
    "    combined_entities = set(spacy_entities + finbert_entities)\n",
    "    \n",
    "    # Post-processing to remove irrelevant entities (example rule: filter out single characters or non-alphanumeric entities)\n",
    "    processed_entities = [entity for entity in combined_entities if len(entity) > 1 and entity.isalnum()]\n",
    "    return ' '.join(processed_entities)\n",
    "\n",
    "def extract_and_preprocess_pdf(pdf_path):\n",
    "    raw_text, _ = extract_text_and_tables_from_pdf(pdf_path)\n",
    "    if raw_text:\n",
    "        preprocessed_text = preprocess_text(raw_text)\n",
    "        \n",
    "        # Extract entities using SpaCy\n",
    "        spacy_entities = extract_entities_spacy(preprocessed_text)\n",
    "        \n",
    "        # Extract entities using FinBERT\n",
    "        finbert_entities = extract_entities_finbert(preprocessed_text)\n",
    "        \n",
    "        # Combine and post-process entities\n",
    "        combined_entities = combine_and_postprocess_entities(spacy_entities, finbert_entities)\n",
    "        \n",
    "        text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "        documents = text_splitter.split_text(combined_entities)\n",
    "        return documents, combined_entities\n",
    "    return [], ''\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_paths = [os.path.join(pdf_folder_path, f) for f in os.listdir(pdf_folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Extract and preprocess documents\n",
    "all_documents = []\n",
    "for pdf_path in pdf_paths:\n",
    "    documents, combined_entities = extract_and_preprocess_pdf(pdf_path)\n",
    "    all_documents.extend(documents)\n",
    "    \n",
    "    # Save the combined entity text as ground truth\n",
    "    pdf_name = os.path.basename(pdf_path).replace('.pdf', '.txt')\n",
    "    ground_truth_path = os.path.join(ground_truth_folder_path, pdf_name)\n",
    "    with open(ground_truth_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_entities)\n",
    "\n",
    "# Save preprocessed documents for later use\n",
    "with open('processed_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.17\n",
      "Average Recall: 1.00\n",
      "Average F1-Score: 0.29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "\n",
    "def load_ground_truth_data(ground_truth_folder_path):\n",
    "    ground_truth_data = {}\n",
    "    for file_name in os.listdir(ground_truth_folder_path):\n",
    "        if file_name.lower().endswith('.txt'):\n",
    "            with open(os.path.join(ground_truth_folder_path, file_name), 'r', encoding='utf-8') as f:\n",
    "                ground_truth_data[file_name] = f.read().lower()\n",
    "    return ground_truth_data\n",
    "\n",
    "def calculate_precision_recall_f1(extracted_text, ground_truth_text):\n",
    "    extracted_tokens = set(extracted_text.split())\n",
    "    ground_truth_tokens = set(ground_truth_text.split())\n",
    "\n",
    "    true_positives = extracted_tokens.intersection(ground_truth_tokens)\n",
    "    false_positives = extracted_tokens.difference(ground_truth_tokens)\n",
    "    false_negatives = ground_truth_tokens.difference(extracted_tokens)\n",
    "\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives)) if (len(true_positives) + len(false_positives)) > 0 else 0\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if (len(true_positives) + len(false_negatives)) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Define the folder containing the ground truth text files\n",
    "ground_truth_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\ground_truth\"\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_data = load_ground_truth_data(ground_truth_folder_path)\n",
    "\n",
    "# Load preprocessed documents\n",
    "with open('processed_documents.pkl', 'rb') as f:\n",
    "    all_documents = pickle.load(f)\n",
    "\n",
    "# Evaluate the extracted documents\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for file_name in ground_truth_data.keys():\n",
    "    ground_truth_text = ground_truth_data[file_name]\n",
    "    extracted_text = ' '.join(all_documents)  # Concatenate all extracted documents for simplicity\n",
    "\n",
    "    precision, recall, f1 = calculate_precision_recall_f1(extracted_text, ground_truth_text)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate average metrics\n",
    "average_precision = sum(precisions) / len(precisions)\n",
    "average_recall = sum(recalls) / len(recalls)\n",
    "average_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(f\"Average Precision: {average_precision:.2f}\")\n",
    "print(f\"Average Recall: {average_recall:.2f}\")\n",
    "print(f\"Average F1-Score: {average_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Using cached stanza-1.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Using cached emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (4.23.4)\n",
      "Requirement already satisfied: requests in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (2.8.4)\n",
      "Requirement already satisfied: toml in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (1.12.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from stanza) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Using cached stanza-1.8.2-py3-none-any.whl (990 kB)\n",
      "Using cached emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.12.1 stanza-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535f4e895a8f4c008aa8322a5927739b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 22:52:01 INFO: Downloaded file to C:\\Users\\WendyChuaXingZhao\\stanza_resources\\resources.json\n",
      "2024-06-09 22:52:01 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9b5dd0d46946f6bb18dcc164659074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 22:52:47 INFO: Downloaded file to C:\\Users\\WendyChuaXingZhao\\stanza_resources\\en\\default.zip\n",
      "2024-06-09 22:52:51 INFO: Finished downloading models and saved to C:\\Users\\WendyChuaXingZhao\\stanza_resources\n",
      "2024-06-09 22:52:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c38d99dbb34123aa50d1b285502705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 22:52:51 INFO: Downloaded file to C:\\Users\\WendyChuaXingZhao\\stanza_resources\\resources.json\n",
      "2024-06-09 22:52:53 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-06-09 22:52:53 INFO: Using device: cpu\n",
      "2024-06-09 22:52:53 INFO: Loading: tokenize\n",
      "2024-06-09 22:52:53 INFO: Loading: mwt\n",
      "2024-06-09 22:52:53 INFO: Loading: pos\n",
      "2024-06-09 22:52:53 INFO: Loading: lemma\n",
      "2024-06-09 22:52:53 INFO: Loading: constituency\n",
      "2024-06-09 22:52:54 INFO: Loading: depparse\n",
      "2024-06-09 22:52:54 INFO: Loading: sentiment\n",
      "2024-06-09 22:52:54 INFO: Loading: ner\n",
      "2024-06-09 22:52:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import pickle\n",
    "import stanza\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Stanza model\n",
    "stanza.download('en')\n",
    "stanza_nlp = stanza.Pipeline('en')\n",
    "\n",
    "# Load FinBERT model for NER\n",
    "model_checkpoint = \"Kansallisarkisto/finbert-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\", framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Define the folder containing the PDF files and the ground truth folder\n",
    "pdf_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\Financial Documents\"\n",
    "ground_truth_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\ground_truth\"\n",
    "\n",
    "# Create ground truth folder if it doesn't exist\n",
    "os.makedirs(ground_truth_folder_path, exist_ok=True)\n",
    "\n",
    "def extract_text_and_tables_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = ''\n",
    "            tables = []\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text\n",
    "                page_tables = page.extract_tables()\n",
    "                if page_tables:\n",
    "                    tables.extend(page_tables)\n",
    "        return full_text, tables\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF {pdf_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def preprocess_text_gensim(text):\n",
    "    # Tokenization and stop-word removal using Gensim\n",
    "    tokens = [token for token in simple_preprocess(text, deacc=True) if token not in STOPWORDS]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_text_stanza(text):\n",
    "    # Use Stanza for preprocessing\n",
    "    doc = stanza_nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words if word.text not in STOPWORDS and re.match(r'[^\\W\\d]*$', word.text)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_entities_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {'PERSON', 'ORG', 'GPE', 'LOC', 'DATE', 'MONEY', 'PERCENT'}]\n",
    "    return entities\n",
    "\n",
    "def extract_entities_finbert(text):\n",
    "    ner_results = token_classifier(text)\n",
    "    entities = [entity['word'] for entity in ner_results]\n",
    "    return entities\n",
    "\n",
    "def combine_and_postprocess_entities(spacy_entities, finbert_entities):\n",
    "    # Combine entities from both models\n",
    "    combined_entities = set(spacy_entities + finbert_entities)\n",
    "    \n",
    "    # Post-processing to remove irrelevant entities (example rule: filter out single characters or non-alphanumeric entities)\n",
    "    processed_entities = [entity for entity in combined_entities if len(entity) > 1 and entity.isalnum()]\n",
    "    return ' '.join(processed_entities)\n",
    "\n",
    "def extract_and_preprocess_pdf(pdf_path, preprocess_method):\n",
    "    raw_text, _ = extract_text_and_tables_from_pdf(pdf_path)\n",
    "    if raw_text:\n",
    "        if preprocess_method == \"gensim\":\n",
    "            preprocessed_text = preprocess_text_gensim(raw_text)\n",
    "        elif preprocess_method == \"stanza\":\n",
    "            preprocessed_text = preprocess_text_stanza(raw_text)\n",
    "        \n",
    "        # Extract entities using SpaCy\n",
    "        spacy_entities = extract_entities_spacy(preprocessed_text)\n",
    "        \n",
    "        # Extract entities using FinBERT\n",
    "        finbert_entities = extract_entities_finbert(preprocessed_text)\n",
    "        \n",
    "        # Combine and post-process entities\n",
    "        combined_entities = combine_and_postprocess_entities(spacy_entities, finbert_entities)\n",
    "        \n",
    "        text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "        documents = text_splitter.split_text(combined_entities)\n",
    "        return documents, combined_entities\n",
    "    return [], ''\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_paths = [os.path.join(pdf_folder_path, f) for f in os.listdir(pdf_folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Extract and preprocess documents using Gensim\n",
    "all_documents_gensim = []\n",
    "for pdf_path in pdf_paths:\n",
    "    documents, combined_entities = extract_and_preprocess_pdf(pdf_path, preprocess_method=\"gensim\")\n",
    "    all_documents_gensim.extend(documents)\n",
    "    \n",
    "    # Save the combined entity text as ground truth\n",
    "    pdf_name = os.path.basename(pdf_path).replace('.pdf', '_gensim.txt')\n",
    "    ground_truth_path = os.path.join(ground_truth_folder_path, pdf_name)\n",
    "    with open(ground_truth_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_entities)\n",
    "\n",
    "# Save preprocessed documents for later use\n",
    "with open('processed_documents_gensim.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents_gensim, f)\n",
    "\n",
    "# Extract and preprocess documents using Stanza\n",
    "all_documents_stanza = []\n",
    "for pdf_path in pdf_paths:\n",
    "    documents, combined_entities = extract_and_preprocess_pdf(pdf_path, preprocess_method=\"stanza\")\n",
    "    all_documents_stanza.extend(documents)\n",
    "    \n",
    "    # Save the combined entity text as ground truth\n",
    "    pdf_name = os.path.basename(pdf_path).replace('.pdf', '_stanza.txt')\n",
    "    ground_truth_path = os.path.join(ground_truth_folder_path, pdf_name)\n",
    "    with open(ground_truth_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_entities)\n",
    "\n",
    "# Save preprocessed documents for later use\n",
    "with open('processed_documents_stanza.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents_stanza, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "\n",
    "def load_ground_truth_data(ground_truth_folder_path, suffix):\n",
    "    ground_truth_data = {}\n",
    "    for file_name in os.listdir(ground_truth_folder_path):\n",
    "        if file_name.lower().endswith(suffix):\n",
    "            with open(os.path.join(ground_truth_folder_path, file_name), 'r', encoding='utf-8') as f:\n",
    "                ground_truth_data[file_name] = f.read().lower()\n",
    "    return ground_truth_data\n",
    "\n",
    "def calculate_precision_recall_f1(extracted_text, ground_truth_text):\n",
    "    extracted_tokens = set(extracted_text.split())\n",
    "    ground_truth_tokens = set(ground_truth_text.split())\n",
    "\n",
    "    true_positives = extracted_tokens.intersection(ground_truth_tokens)\n",
    "    false_positives = extracted_tokens.difference(ground_truth_tokens)\n",
    "    false_negatives = ground_truth_tokens.difference(extracted_tokens)\n",
    "\n",
    "    precision = len(true_positives) / (len(true_positives) + len(false_positives)) if (len(true_positives) + len(false_positives)) > 0 else 0\n",
    "    recall = len(true_positives) / (len(true_positives) + len(false_negatives)) if (len(true_positives) + len(false_negatives)) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Define the folder containing the ground truth text files\n",
    "ground_truth_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\ground_truth\"\n",
    "\n",
    "# Load ground truth data for gensim\n",
    "ground_truth_data_gensim = load_ground_truth_data(ground_truth_folder_path, \"_gensim.txt\")\n",
    "\n",
    "# Load preprocessed documents for gensim\n",
    "with open('processed_documents_gensim.pkl', 'rb') as f:\n",
    "    all_documents_gensim = pickle.load(f)\n",
    "\n",
    "# Evaluate the extracted documents for gensim\n",
    "precisions_gensim = []\n",
    "recalls_gensim = []\n",
    "f1_scores_gensim = []\n",
    "\n",
    "for file_name in ground_truth_data_gensim.keys():\n",
    "    ground_truth_text = ground_truth_data_gensim[file_name]\n",
    "    extracted_text = ' '.join(all_documents_gensim)  # Concatenate all extracted documents for simplicity\n",
    "\n",
    "    precision, recall, f1 = calculate_precision_recall_f1(extracted_text, ground_truth_text)\n",
    "    precisions_gensim.append(precision)\n",
    "    recalls_gensim.append(recall)\n",
    "    f1_scores_gensim.append(f1)\n",
    "\n",
    "# Calculate average metrics for gensim\n",
    "average_precision_gensim = sum(precisions_gensim) / len(precisions_gensim)\n",
    "average_recall_gensim = sum(recalls_gensim) / len(recalls_gensim)\n",
    "average_f1_gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='error_log.log', filemode='w',\n",
    "                    format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load preprocessed documents\n",
    "with open('processed_documents.pkl', 'rb') as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes: Optional[List[str]] = None,\n",
    "    rels: Optional[List[str]] = None\n",
    ") -> None:\n",
    "    try:\n",
    "        # Extract graph data using OpenAI functions\n",
    "        extract_chain = get_extraction_chain(nodes, rels)\n",
    "        response = extract_chain.invoke(document)\n",
    "        \n",
    "        # Log the response for debugging\n",
    "        logging.info(\"Response from extract_chain: %s\", response)\n",
    "        \n",
    "        # Validate JSON response\n",
    "        try:\n",
    "            data = response['function']\n",
    "            # Construct a graph document\n",
    "            graph_document = GraphDocument(\n",
    "                nodes=[map_to_base_node(node) for node in data.nodes],\n",
    "                relationships=[map_to_base_relationship(rel) for rel in data.rels],\n",
    "                source=document.dict()  # Convert Document object to dictionary\n",
    "            )\n",
    "            \n",
    "            # Store information into a graph\n",
    "            graph.add_graph_documents([graph_document])\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(\"JSON decoding error: %s\", e)\n",
    "            logging.error(\"Invalid JSON response: %s\", response)\n",
    "        except KeyError as e:\n",
    "            logging.error(\"Key error: %s\", e)\n",
    "            logging.error(\"Response missing expected key: %s\", response)\n",
    "        except Exception as e:\n",
    "            logging.error(\"An unexpected error occurred: %s\", e)\n",
    "            logging.error(\"Response: %s\", response)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occurred in extract_and_store_graph: %s\", e)\n",
    "\n",
    "# Process each document chunk and extract/store the graph\n",
    "for i, chunk in tqdm(enumerate(documents), total=len(documents)):\n",
    "    doc = Document(page_content=chunk, metadata={\"source\": \"batch_process\", \"chunk\": i})\n",
    "    try:\n",
    "        extract_and_store_graph(doc)\n",
    "        print(f\"Successfully processed and stored chunk {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing chunk {i+1}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
