{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: neo4j in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (5.20.0)\n",
      "Requirement already satisfied: openai in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.30.1)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (1.24.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.6.6)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (0.1.60)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (2.5.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: pytz in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from neo4j) (2022.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from PyMuPDF) (1.24.3)\n",
      "Requirement already satisfied: click in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.14.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\wendychuaxingzhao\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain neo4j openai wikipedia tiktoken langchain_openai pdfplumber PyMuPDF nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package Imports\n",
    "from langchain.graphs import Neo4jGraph\n",
    "import neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neo4j connection\n",
    "url = \"neo4j+s://39edf771.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"31Nwe5MwJKLGHFCTtkmWQVO7R3DU1fYYvX_D63HZGEM\"\n",
    "graph = Neo4jGraph(\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain for graph\n",
    "from langchain_community.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "\n",
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )\n",
    "\n",
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WendyChuaXingZhao\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# The base URL for your Azure OpenAI resource.\n",
    "openai_api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# API version e.g. \"2023-07-01-preview\"\n",
    "openai_api_version = config_details['OPENAI_API_VERSION']\n",
    "\n",
    "# The name of your Azure OpenAI deployment chat model. e.g. \"gpt-35-turbo-0613\"\n",
    "deployment_name = config_details['DEPLOYMENT_NAME']\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is set to `azure`\n",
    "openai_api_type = \"azure\"\n",
    "\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "# Create an instance of chat llm\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=openai_api_base,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=deployment_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type=openai_api_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "      \"system\",\n",
    "      f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a specialized algorithm designed to extract structured financial data from bank annual reports to build a comprehensive knowledge graph.\n",
    "- **Nodes** represent financial terms, entities, departments, and concepts specific to the banking industry.\n",
    "- The goal is to structure information in a manner that highlights financial relationships, decision-making hierarchies, and policy summaries.\n",
    "\n",
    "## 2. Sections and Nodes\n",
    "- **Sections**: Target key sections such as 'Director's Report', 'Statements of Financial Position', 'Income Statement', etc. Identify relevant financial data and decisions.\n",
    "- **Entities and Concepts**: Create nodes for identifiable entities like board members, financial metrics, and policy terms.\n",
    "- **Node IDs**: Use names or specific identifiers for node labels. Avoid integers or vague references.\n",
    "- **Relationships**: Map relationships that reflect financial dependencies, reporting structures, and policy impacts.\n",
    "\n",
    "## 3. Detailed Financial Extraction\n",
    "- **Extract Details**: Focus on numbers and financial statements, converting them into properties of nodes. For instance, assets and liabilities figures should be attached to the 'Statement of Financial Position' node.\n",
    "- **Accuracy and Precision**: Ensure the extraction of financial data is accurate, citing exact figures and contextual information.\n",
    "- **Contextual Relevance**: Attach relevant notes and assumptions from the 'Notes to Financial Statements' to the appropriate financial statements or metrics.\n",
    "\n",
    "## 4. Compliance and Consistency\n",
    "- **Regulatory Statements**: Extract and highlight compliance statements from 'Independent Auditors' Report' and 'Statutory Declaration'.\n",
    "- **Maintain Consistency**: Use consistent terminology across different sections of the report to avoid confusion.\n",
    "- **Strict Compliance**: Adhere strictly to the rules for knowledge graph construction.\n",
    "\n",
    "## 5. Symbolic Tokens and Special Characters\n",
    "- **Symbolic Tokens**: Identify and properly handle symbolic tokens (e.g., %, $, £, etc.) and ensure they are accurately represented in the data.\n",
    "- **Special Characters**: Recognize special characters and their meanings (e.g., ± for approximately, > for greater than, etc.) and handle them appropriately in the extraction process.\n",
    "\n",
    "## 6. Domain Vocabulary and Abbreviations\n",
    "- **Domain Vocabulary**: Utilize and recognize domain-specific vocabulary and terminology relevant to the banking and financial industry.\n",
    "- **Abbreviations**: Identify and expand domain-related abbreviations (e.g., FVOCI for Fair Value through Other Comprehensive Income) to ensure clarity and accuracy.\n",
    "\n",
    "## 7. N-gram Extraction\n",
    "- **N-grams**: Extract meaningful N-grams (bigrams, trigrams, etc.) that represent significant financial terms or phrases and include them as part of the node properties.\n",
    "\n",
    "## 8. Tips\n",
    "- Remember to format financial data as attributes of the nodes and structure the graph to reflect the organization and flow of the annual report.\n",
    "- Use the given format to extract information from the following input: <input here>\n",
    "- Tip: Make sure to answer in the correct format\n",
    "\"\"\"),\n",
    "        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "        (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "    ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes: Optional[List[str]] = None,\n",
    "    rels: Optional[List[str]] = None\n",
    ") -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.invoke(document)['function']\n",
    "    \n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "        nodes=[map_to_base_node(node) for node in data.nodes],\n",
    "        relationships=[map_to_base_relationship(rel) for rel in data.rels],\n",
    "        source=document.dict()  # Convert Document object to dictionary\n",
    "    )\n",
    "    \n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WendyChuaXingZhao\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: cimb-fs-2023.txt\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n",
      "\n",
      "\n",
      "Document: cimb-integrated-report-2023.txt\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n",
      "\n",
      "\n",
      "Document: Maybank-AR2023-Financial-Statements.txt\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n",
      "\n",
      "\n",
      "Document: Maybank-AR2023-Integrated-Annual-Report.txt\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n",
      "\n",
      "\n",
      "Document: PBB_ar_2023.txt\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n",
      "\n",
      "\n",
      "Document: PBB_fs_2023.txt\n",
      "Precision: 0\n",
      "Recall: 0\n",
      "F1-score: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the folder containing the PDF files and the ground truth folder\n",
    "pdf_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\Financial Documents\"\n",
    "ground_truth_folder_path = r\"C:\\Users\\WendyChuaXingZhao\\OneDrive - SRKK Group of Companies\\Documents\\MsDS\\Knowledge Graph Test\\ground_truth\"\n",
    "\n",
    "# Create ground truth folder if it doesn't exist\n",
    "os.makedirs(ground_truth_folder_path, exist_ok=True)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[\\s]+', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "def extract_and_preprocess_pdf(pdf_path):\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    if raw_text:\n",
    "        preprocessed_text = preprocess_text(raw_text)\n",
    "        text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
    "        documents = text_splitter.split_text(preprocessed_text)\n",
    "        return documents, preprocessed_text\n",
    "    return [], None\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_paths = [os.path.join(pdf_folder_path, f) for f in os.listdir(pdf_folder_path) if f.lower().endswith('.pdf')]\n",
    "\n",
    "# Extract and preprocess documents\n",
    "all_documents = []\n",
    "for pdf_path in pdf_paths:\n",
    "    documents, preprocessed_text = extract_and_preprocess_pdf(pdf_path)\n",
    "    all_documents.extend(documents)\n",
    "    \n",
    "    # Save the preprocessed text as ground truth\n",
    "    if preprocessed_text:\n",
    "        pdf_name = os.path.basename(pdf_path).replace('.pdf', '.txt')\n",
    "        ground_truth_path = os.path.join(ground_truth_folder_path, pdf_name)\n",
    "        with open(ground_truth_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(preprocessed_text)\n",
    "\n",
    "# Save preprocessed documents for later use\n",
    "with open('processed_documents.pkl', 'wb') as f:\n",
    "    pickle.dump(all_documents, f)\n",
    "\n",
    "# Load ground truth data and extracted data for evaluation\n",
    "def load_text_files(folder_path):\n",
    "    text_data = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                text_data[file_name] = preprocess_text(text)\n",
    "    return text_data\n",
    "\n",
    "ground_truth_data = load_text_files(ground_truth_folder_path)\n",
    "\n",
    "def load_extracted_data(pickle_file_path):\n",
    "    with open(pickle_file_path, 'rb') as f:\n",
    "        extracted_data = pickle.load(f)\n",
    "    return extracted_data\n",
    "\n",
    "extracted_data_path = 'processed_documents.pkl'\n",
    "extracted_data_raw = load_extracted_data(extracted_data_path)\n",
    "\n",
    "# Convert the extracted data into a dictionary with filenames as keys for comparison\n",
    "extracted_data = {}\n",
    "for i, document in enumerate(extracted_data_raw):\n",
    "    doc_name = f'document_{i+1}.txt'\n",
    "    extracted_data[doc_name] = ' '.join(document)\n",
    "\n",
    "def evaluate_extraction(ground_truth, extracted):\n",
    "    \"\"\"\n",
    "    Evaluate the extraction against the ground truth.\n",
    "    \n",
    "    :param ground_truth: Dictionary containing ground truth data\n",
    "    :param extracted: Dictionary containing extracted data\n",
    "    :return: Dictionary containing precision, recall, and F1-score for each document\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for doc, true_text in ground_truth.items():\n",
    "        if doc in extracted:\n",
    "            extracted_text = extracted[doc]\n",
    "            true_words = set(true_text.split())\n",
    "            extracted_words = set(extracted_text.split())\n",
    "            tp = len(true_words & extracted_words)\n",
    "            fp = len(extracted_words - true_words)\n",
    "            fn = len(true_words - extracted_words)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            results[doc] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "        else:\n",
    "            results[doc] = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "    return results\n",
    "\n",
    "# Evaluate the extracted data against the ground truth data\n",
    "evaluation_results = evaluate_extraction(ground_truth_data, extracted_data)\n",
    "\n",
    "# Print evaluation results\n",
    "for doc, metrics in evaluation_results.items():\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"Precision: {metrics['precision']}\")\n",
    "    print(f\"Recall: {metrics['recall']}\")\n",
    "    print(f\"F1-score: {metrics['f1']}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/53 [00:00<?, ?it/s]c:\\Users\\WendyChuaXingZhao\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The function `create_structured_output_chain` was deprecated in LangChain 0.1.1 and will be removed in 0.3.0. Use ChatOpenAI.with_structured_output instead.\n",
      "  warn_deprecated(\n",
      "  2%|▏         | 1/53 [00:11<10:00, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 1: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/53 [03:19<1:37:49, 115.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/53 [03:43<1:01:30, 73.80s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 3: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/53 [05:52<1:17:55, 95.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 4: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/53 [07:43<1:20:58, 101.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 6/53 [11:05<1:45:56, 135.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 6: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/53 [11:29<1:15:57, 99.07s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 8/53 [11:59<57:41, 76.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 9/53 [12:08<40:58, 55.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 10/53 [12:45<35:50, 50.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 11/53 [13:09<29:18, 41.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 12/53 [16:26<1:00:54, 89.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 12: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 13/53 [17:01<48:26, 72.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 14/53 [17:34<39:29, 60.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 15/53 [17:51<30:14, 47.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 16/53 [18:53<32:04, 52.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 17/53 [19:28<27:59, 46.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 18/53 [19:28<19:07, 32.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 18: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 19/53 [20:20<21:50, 38.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 20/53 [20:35<17:20, 31.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 21/53 [23:11<36:42, 68.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 22/53 [26:23<54:39, 105.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 23/53 [26:55<41:50, 83.70s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 23: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 24/53 [27:05<29:40, 61.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 25/53 [27:10<20:51, 44.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 26/53 [27:22<15:39, 34.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 27/53 [28:17<17:38, 40.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 28/53 [28:47<15:38, 37.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 29/53 [29:17<14:10, 35.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 29: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 30/53 [29:33<11:18, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 31/53 [29:37<08:02, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 32/53 [30:38<11:43, 33.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 33/53 [33:49<26:54, 80.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 33: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 34/53 [34:30<21:48, 68.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 35/53 [34:39<15:16, 50.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 36/53 [35:05<12:20, 43.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 36: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 37/53 [35:34<10:26, 39.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 38/53 [36:47<12:20, 49.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 39/53 [36:51<08:19, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 40/53 [37:02<06:08, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 41/53 [37:11<04:29, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 42/53 [37:47<04:53, 26.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 43/53 [38:54<06:26, 38.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 44/53 [42:07<12:44, 84.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 44: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 45/53 [42:27<08:44, 65.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 46/53 [42:38<05:43, 49.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 47/53 [42:48<03:43, 37.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 48/53 [43:22<03:02, 36.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 49/53 [44:34<03:08, 47.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 50/53 [45:04<02:05, 41.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing chunk 50: local variable 'response' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 51/53 [45:10<01:02, 31.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 52/53 [45:40<00:30, 30.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [45:59<00:00, 52.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and stored chunk 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='error_log.log', filemode='w',\n",
    "                    format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def extract_and_store_graph(document: Document, nodes: Optional[List[str]] = None, rels: Optional[List[str]] = None) -> None:\n",
    "    try:\n",
    "        # Extract graph data using OpenAI functions\n",
    "        extract_chain = get_extraction_chain(nodes, rels)\n",
    "        response = extract_chain.invoke(document)\n",
    "        \n",
    "        # Log the response for debugging\n",
    "        logging.info(\"Response from extract_chain: %s\", response)\n",
    "        \n",
    "        data = response['function']\n",
    "        \n",
    "        # Construct a graph document\n",
    "        graph_document = GraphDocument(\n",
    "            nodes=[map_to_base_node(node) for node in data.nodes],\n",
    "            relationships=[map_to_base_relationship(rel) for rel in data.rels],\n",
    "            source=document.dict()  # Convert Document object to dictionary\n",
    "        )\n",
    "        \n",
    "        # Store information into a graph\n",
    "        graph.add_graph_documents([graph_document])\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(\"JSON decoding error: %s\", e)\n",
    "        logging.error(\"Invalid JSON response: %s\", response)\n",
    "    except KeyError as e:\n",
    "        logging.error(\"Key error: %s\", e)\n",
    "        logging.error(\"Response missing expected key: %s\", response)\n",
    "    except Exception as e:\n",
    "        logging.error(\"An unexpected error occurred: %s\", e)\n",
    "        logging.error(\"Response: %s\", response)\n",
    "\n",
    "# Process each document chunk and extract/store the graph\n",
    "for i, chunk in tqdm(enumerate(documents), total=len(documents)):\n",
    "    doc = Document(page_content=chunk, metadata={\"source\": \"batch_process\", \"chunk\": i})\n",
    "    try:\n",
    "        extract_and_store_graph(doc)\n",
    "        print(f\"Successfully processed and stored chunk {i+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing chunk {i+1}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Person)-[:FOUNDED]->(b:Bank {name: \"Maybank\"})\n",
      "RETURN p.name AS founder\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'founder': 'Tan_Sri_Khoo_Teck_Puat'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The founder of Maybank is Tan Sri Khoo Teck Puat.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the knowledge graph in a RAG application\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph,\n",
    "    cypher_llm=AzureChatOpenAI(\n",
    "    azure_endpoint=openai_api_base,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=\"knowledgegraphGPT4\",\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type=openai_api_type,\n",
    "),\n",
    "    qa_llm=AzureChatOpenAI(\n",
    "    azure_endpoint=openai_api_base,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=deployment_name,\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_type=openai_api_type,\n",
    "),\n",
    "    validate_cypher=True, # Validate relationship directions\n",
    "    verbose=True\n",
    ")\n",
    "cypher_chain.run(\"Who is the founder of Maybank?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
